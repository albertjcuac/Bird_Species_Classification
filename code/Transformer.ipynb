{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importes","metadata":{}},{"cell_type":"code","source":"## Standard libraries\nimport os\nimport glob\nimport numpy as np\nimport random\nimport math\nimport json\nfrom functools import partial\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nimport pandas as pd\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n\n## Torchvision\nimport torchvision\nfrom torchvision import transforms\nimport PIL\nimport torchvision.transforms as T\n\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError:\n    !pip install --quiet pytorch-lightning>=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n# Setting the seed\npl.seed_everything(42)\n\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\nTEST_PATH= \"../input/iais22-birds/submission_test/submission_test\"\nTRAIN_PATH = \"../input/iais22-birds/birds/birds\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T11:03:21.574306Z","iopub.execute_input":"2022-06-27T11:03:21.575226Z","iopub.status.idle":"2022-06-27T11:03:21.591692Z","shell.execute_reply.started":"2022-06-27T11:03:21.575170Z","shell.execute_reply":"2022-06-27T11:03:21.590610Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"# PREPARACIÓN DATASET Y DATALOADER DE ENTRENAMIENTO","metadata":{}},{"cell_type":"markdown","source":"Mediante esta función hemos calculado la media y la desviación típica de nuestro dataset","metadata":{}},{"cell_type":"code","source":"def mean_std(loader):\n    images, lebels = next(iter(loader))\n  # shape of images = [b,c,w,h]\n    mean, std = images.mean([0,2,3]), images.std([0,2,3])\n    return mean, std","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:21.593582Z","iopub.execute_input":"2022-06-27T11:03:21.594300Z","iopub.status.idle":"2022-06-27T11:03:21.604572Z","shell.execute_reply.started":"2022-06-27T11:03:21.594261Z","shell.execute_reply":"2022-06-27T11:03:21.603537Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"Definimos las transformaciones de nuestros datos para hacer más efiente el entrenamiento","metadata":{}},{"cell_type":"code","source":"# Para el entrenamiento y el test, añadimos algún aumento. Las redes son demasiado potentes y se pueden sobreajustar.\n\ntest_transform = transforms.Compose([transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize([0.4740, 0.4676, 0.4134], [0.2143, 0.2085, 0.2333])\n                                     ])\n\ntrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.4740, 0.4676, 0.4134], [0.2143, 0.2085, 0.2333])\n                                     ])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:21.606094Z","iopub.execute_input":"2022-06-27T11:03:21.606622Z","iopub.status.idle":"2022-06-27T11:03:21.617361Z","shell.execute_reply.started":"2022-06-27T11:03:21.606582Z","shell.execute_reply":"2022-06-27T11:03:21.616396Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"Definimos nuestros datasets y dataloaders, tanto para el entrenamiento como para la validación","metadata":{}},{"cell_type":"code","source":"train_dataset=torchvision.datasets.ImageFolder(root=TRAIN_PATH,transform=train_transform)\nval_dataset=torchvision.datasets.ImageFolder(root=TRAIN_PATH,transform=test_transform)\ntrain_dataset_split,_ = torch.utils.data.random_split(train_dataset,[48000,10388])\n_,val_dataset_split = torch.utils.data.random_split(val_dataset,[48000,10388])\nloader= torch.utils.data.DataLoader(train_dataset_split,batch_size=128,shuffle=True, drop_last=True, pin_memory=True, num_workers=2)\nval_loader = data.DataLoader(val_dataset_split, batch_size=128, shuffle=False, drop_last=False, num_workers=2)\nbatch=next(iter(loader))\nimgs, labels= batch\nimgs.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:21.621549Z","iopub.execute_input":"2022-06-27T11:03:21.622489Z","iopub.status.idle":"2022-06-27T11:03:23.709352Z","shell.execute_reply.started":"2022-06-27T11:03:21.621790Z","shell.execute_reply":"2022-06-27T11:03:23.708372Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# PATCHEMBEDDING","metadata":{}},{"cell_type":"markdown","source":"Definimos nuestra función para dividir la imagen en parches","metadata":{}},{"cell_type":"code","source":"def img_to_patch(x, patch_size, flatten_channels=True):\n    \"\"\"\n    Inputs:\n        x - torch.Tensor que representa la imagen de la forma [B, C, H, W]\n        patch_size - Número de píxeles por dimensión de los parches (integer)\n        flatten_channels - Si es True, los parches se devolverán en un formato aplanado\n                           como un vector de características en lugar de una cuadrícula de imágenes.\n    \"\"\"\n    B, C, H, W = x.shape\n    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n    if flatten_channels:\n        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:23.711191Z","iopub.execute_input":"2022-06-27T11:03:23.711607Z","iopub.status.idle":"2022-06-27T11:03:23.718544Z","shell.execute_reply.started":"2022-06-27T11:03:23.711564Z","shell.execute_reply":"2022-06-27T11:03:23.717654Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Mostramos la información que obtenemos tras realizar el patch embedding, en este caso hemos cogido 4 imagénes de 32x32 píxeles y las dividiremos en parches de 4x4 píxeles, dando lugar a 64 parches","metadata":{}},{"cell_type":"code","source":"NUM_IMAGES=4\nBIRD_images = torch.stack([train_dataset[idx][0] for idx in range(NUM_IMAGES)], dim=0)\nimg_patches = img_to_patch(BIRD_images, patch_size=4, flatten_channels=False)\n\nfig, ax = plt.subplots(BIRD_images.shape[0], 1, figsize=(14,3))\nfig.suptitle(\"Imágenes como secuencias de entrada de parches\")\nfor i in range(BIRD_images.shape[0]):\n    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n    img_grid = img_grid.permute(1, 2, 0)\n    ax[i].imshow(img_grid)\n    ax[i].axis('off')\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:23.720370Z","iopub.execute_input":"2022-06-27T11:03:23.720861Z","iopub.status.idle":"2022-06-27T11:03:24.041839Z","shell.execute_reply.started":"2022-06-27T11:03:23.720821Z","shell.execute_reply":"2022-06-27T11:03:24.040511Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"# ATTENTION","metadata":{}},{"cell_type":"markdown","source":"Ahora procedemos a crear el algoritmo de atención, para ello comenzamos implementando el mecanismo descrito en nuestro artículo científico. Aun teniendo creado tanto nuestra función de producto escalar como la de Atención Multi-Cabezas no fuimos capaz de utilizarla debido a un fallo que no fuimos capaces de comprender, a pesar de esto logramos implementar el bloque de atención donde aplicamos el módulo nn.MultiHeadAttention. ","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.045361Z","iopub.execute_input":"2022-06-27T11:03:24.052839Z","iopub.status.idle":"2022-06-27T11:03:24.068311Z","shell.execute_reply.started":"2022-06-27T11:03:24.052799Z","shell.execute_reply":"2022-06-27T11:03:24.067552Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n\n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"La dimensión de embedding debe tener un valor 0 mod num_heads\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, embed_dim = x.size()\n        qkv = self.qkv_proj(x)\n\n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, embed_dim)\n        o = self.o_proj(values)\n\n        if return_attention:\n            return o, attention\n        else:\n            return o","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.074511Z","iopub.execute_input":"2022-06-27T11:03:24.076843Z","iopub.status.idle":"2022-06-27T11:03:24.097701Z","shell.execute_reply.started":"2022-06-27T11:03:24.076805Z","shell.execute_reply":"2022-06-27T11:03:24.096744Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n\n    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.12):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionalidad de los vectores de entrada y atención\n            hidden_dim - Dimensionalidad de la capa oculta en la red feed-forward\n                         (normalmente 2-4 veces mayor que embed_dim)\n            num_heads - Número de cabezas a utilizar en el bloque de Atención multicabeza\n            dropout -  Cantidad de dropout a aplicar en la red feed-forward\n        \"\"\"\n        super().__init__()\n\n        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n                                          dropout=dropout)\n        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n        self.linear = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n\n    def forward(self, x):\n        inp_x = self.layer_norm_1(x)\n        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n        x = x + self.linear(self.layer_norm_2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.101798Z","iopub.execute_input":"2022-06-27T11:03:24.103958Z","iopub.status.idle":"2022-06-27T11:03:24.117586Z","shell.execute_reply.started":"2022-06-27T11:03:24.103918Z","shell.execute_reply":"2022-06-27T11:03:24.116858Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"# CREACIÓN DEL MODELO","metadata":{}},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n\n    def __init__(self, embed_dim=256, hidden_dim=512, num_channels=3, num_heads=8, num_layers=6, num_classes=400, \n                 patch_size=4, num_patches=64, dropout=0.12):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionalidad de los vectores de entrada del Transformer\n            hidden_dim - Dimensionalidad de la capa oculta en las redes feed-forward del Transformer\n            num_channels - Número de canales del input (3 para RGB)\n            num_heads - Número de cabezas a usar en el bloque de Atención Multi-Cabeza\n            num_layers - Número de capas a usar en el Transformer\n            num_classes - Número de clases a predecir\n            patch_size - Numero de pixeles que el parche tiene por dimensión\n            num_patches - Máximo número de parches que el Transformer puede tener\n            dropout - Cantidad de dropout a aplicar en la red feed-forward y\n                      en la codificación de entrada\n        \"\"\"\n        super().__init__()\n\n        self.patch_size = patch_size\n\n        # Layers/Networks\n        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n        # Parameters/Embeddings\n        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n\n\n    def forward(self, x):\n        # Preprocess input\n        x = img_to_patch(x, self.patch_size)\n        B, T, _ = x.shape\n        x = self.input_layer(x)\n\n        # Add CLS token and positional encoding\n        cls_token = self.cls_token.repeat(B, 1, 1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = x + self.pos_embedding[:,:T+1]\n\n        # Apply Transformer\n        x = self.dropout(x)\n        x = x.transpose(0, 1)\n        x = self.transformer(x)\n\n        # Perform classification prediction\n        cls = x[0]\n        out = self.mlp_head(cls)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.120776Z","iopub.execute_input":"2022-06-27T11:03:24.121414Z","iopub.status.idle":"2022-06-27T11:03:24.146651Z","shell.execute_reply.started":"2022-06-27T11:03:24.121296Z","shell.execute_reply":"2022-06-27T11:03:24.145660Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"class ViT(pl.LightningModule):\n\n    def __init__(self, lr=3e-4):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = VisionTransformer().to(device)\n        self.example_input_array = next(iter(loader))[0]\n\n    def forward(self, x):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n        return [optimizer], [lr_scheduler]\n        \n    def predict(self, x):\n        with torch.no_grad():\n            y_hat = self(x)\n            return torch.argmax(y_hat, axis=1)\n\n    def _calculate_loss(self, batch, mode=\"train\"):\n        imgs, labels = batch[0].to(device),batch[1].to(device)\n        preds = self.model(imgs)\n        loss = F.cross_entropy(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        self.log(f'{mode}_loss', loss, prog_bar=True)\n        self.log(f'{mode}_acc', acc, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._calculate_loss(batch, mode=\"train\")\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"val\")\n\n    def test_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.150889Z","iopub.execute_input":"2022-06-27T11:03:24.156236Z","iopub.status.idle":"2022-06-27T11:03:24.175237Z","shell.execute_reply.started":"2022-06-27T11:03:24.156195Z","shell.execute_reply":"2022-06-27T11:03:24.174163Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# ENTRENAMIENTO Y GUARDADO DEL MODELO\n","metadata":{}},{"cell_type":"code","source":"model = ViT().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:24.180540Z","iopub.execute_input":"2022-06-27T11:03:24.180923Z","iopub.status.idle":"2022-06-27T11:03:25.838601Z","shell.execute_reply.started":"2022-06-27T11:03:24.180884Z","shell.execute_reply":"2022-06-27T11:03:25.837553Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(42)\ntrainer = pl.Trainer(\n                     accelerator='gpu',\n                     gpus=1 if str(device)==\"cuda:0\" else 0,\n                     max_epochs=100,\n)\ntrainer.fit(model, loader, val_loader)\ntrainer.logger._default_hp_metric = None # Optional logging argument that we don't need","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:03:25.840236Z","iopub.execute_input":"2022-06-27T11:03:25.840649Z","iopub.status.idle":"2022-06-27T11:04:25.173671Z","shell.execute_reply.started":"2022-06-27T11:03:25.840605Z","shell.execute_reply":"2022-06-27T11:04:25.170525Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"val_result = trainer.test(model, val_loader, verbose=False)\nresult = {\"val\": val_result[0][\"test_acc\"]}\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:04:25.177531Z","iopub.execute_input":"2022-06-27T11:04:25.188676Z","iopub.status.idle":"2022-06-27T11:04:40.145902Z","shell.execute_reply.started":"2022-06-27T11:04:25.188626Z","shell.execute_reply":"2022-06-27T11:04:40.144299Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"state_dict = model.state_dict()\n\n# torch.save(object, filename). For the filename, any extension can be used\ntorch.save(state_dict, \"bird_transformer.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:04:42.354653Z","iopub.execute_input":"2022-06-27T11:04:42.355596Z","iopub.status.idle":"2022-06-27T11:04:42.408913Z","shell.execute_reply.started":"2022-06-27T11:04:42.355548Z","shell.execute_reply":"2022-06-27T11:04:42.408061Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"# CARGAR MODELO Y REALIZAR PREDDICIONES SOBRE DATOS DE TEST(GUARDADOS EN CSV)","metadata":{}},{"cell_type":"code","source":"state_dict2 = torch.load(\"../input/modelos/bird_transformer_bueno (1).pth\")\n\n\nnew_model = ViT()\nnew_model.load_state_dict(state_dict2)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:04:54.971317Z","iopub.execute_input":"2022-06-27T11:04:54.972525Z","iopub.status.idle":"2022-06-27T11:04:56.630282Z","shell.execute_reply.started":"2022-06-27T11:04:54.972478Z","shell.execute_reply":"2022-06-27T11:04:56.629138Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(\n                     accelerator=\"gpu\",\n                     gpus=1 if str(device)==\"cuda:0\" else 0,\n                     max_epochs=180,\n)\ntrainer.fit(new_model, loader,val_loader)\ntrainer.logger._default_hp_metric = None # Optional logging argument that we don't need","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:04:40.153393Z","iopub.status.idle":"2022-06-27T11:04:40.154257Z","shell.execute_reply.started":"2022-06-27T11:04:40.154012Z","shell.execute_reply":"2022-06-27T11:04:40.154037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, list_IDs):\n        \n        self.list_IDs = list_IDs\n\n    def __len__(self):\n        \n        return len(self.list_IDs)\n\n    def __getitem__(self, index):\n        \n        # Select sample\n        ID = self.list_IDs[index]\n\n        # Load data\n        img = Image.open(ID)\n        X = test_transform(img).unsqueeze(0)\n\n\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:07.071082Z","iopub.execute_input":"2022-06-27T11:05:07.071520Z","iopub.status.idle":"2022-06-27T11:05:07.078951Z","shell.execute_reply.started":"2022-06-27T11:05:07.071481Z","shell.execute_reply":"2022-06-27T11:05:07.078019Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"classes=[]\nids=[]\ntest_image_paths = []","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:09.350329Z","iopub.execute_input":"2022-06-27T11:05:09.351116Z","iopub.status.idle":"2022-06-27T11:05:09.355737Z","shell.execute_reply.started":"2022-06-27T11:05:09.351079Z","shell.execute_reply":"2022-06-27T11:05:09.354399Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"classes = os.listdir(TRAIN_PATH)\nids = os.listdir(TEST_PATH)\n\nfor data_path in glob.glob(TEST_PATH + '/*'):\n    test_image_paths.append(data_path)\n\ntest_dataset=Dataset(test_image_paths)\nclasses=sorted(classes)\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:10.790272Z","iopub.execute_input":"2022-06-27T11:05:10.790908Z","iopub.status.idle":"2022-06-27T11:05:10.809280Z","shell.execute_reply.started":"2022-06-27T11:05:10.790867Z","shell.execute_reply":"2022-06-27T11:05:10.807911Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"test_loader= torch.utils.data.DataLoader(test_dataset,batch_size=2000, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:27.940626Z","iopub.execute_input":"2022-06-27T11:05:27.941349Z","iopub.status.idle":"2022-06-27T11:05:27.946669Z","shell.execute_reply.started":"2022-06-27T11:05:27.941307Z","shell.execute_reply":"2022-06-27T11:05:27.945285Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"batch=next(iter(test_loader))\nimgs = batch.to(device)\ntest_imgs = torch.squeeze(imgs, 1)\ntest_imgs.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:29.064774Z","iopub.execute_input":"2022-06-27T11:05:29.065630Z","iopub.status.idle":"2022-06-27T11:05:34.531853Z","shell.execute_reply.started":"2022-06-27T11:05:29.065592Z","shell.execute_reply":"2022-06-27T11:05:34.530850Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Para poder realizar el siguiente paso, no es posible realizarlo en kaggle debido a limitaciones de memoria que no hemos sabido como solucionar, para poder ejecutar la siguiente celda aconsejamos ejecutarla en un entorno con suficiente memoria en la GPU o ejecutar la siguiente celda donde realizamos las pruebas de una manera menos eficiente","metadata":{}},{"cell_type":"code","source":"outputs = new_model(test_imgs)\n_, predicted = torch.max(outputs, 1)\npred_classes=[]\nfor index in predicted:\n    pred_classes.append(classes[index])\n    \nprint('Predicted: ', ' '.join(f'{predicted[j]:5d}' for j in range(2000)))","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:05:38.100584Z","iopub.execute_input":"2022-06-27T11:05:38.101200Z","iopub.status.idle":"2022-06-27T11:05:38.460321Z","shell.execute_reply.started":"2022-06-27T11:05:38.101162Z","shell.execute_reply":"2022-06-27T11:05:38.458994Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"pred_classes = []\nfor path in test_image_paths:\n    img = PIL.Image.open(path)\n    img_tensor = test_transform(img).unsqueeze(0)\n    pred=new_model.predict(img_tensor.to(device))\n    pred_classes.append(classes[pred])\nprint(pred_classes)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:15:46.593532Z","iopub.execute_input":"2022-06-27T11:15:46.594121Z","iopub.status.idle":"2022-06-27T11:15:59.608218Z","shell.execute_reply.started":"2022-06-27T11:15:46.594085Z","shell.execute_reply":"2022-06-27T11:15:59.605636Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"ids_replace=[]\nfor id in ids:\n    ids_replace.append(id.replace('.jpg',''))\ndic={'Id':ids_replace,'Category':pred_classes}\ndf=pd.DataFrame(data=dic)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:16:21.499828Z","iopub.execute_input":"2022-06-27T11:16:21.500881Z","iopub.status.idle":"2022-06-27T11:16:21.517784Z","shell.execute_reply.started":"2022-06-27T11:16:21.500834Z","shell.execute_reply":"2022-06-27T11:16:21.516983Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T11:16:23.980398Z","iopub.execute_input":"2022-06-27T11:16:23.981200Z","iopub.status.idle":"2022-06-27T11:16:23.992319Z","shell.execute_reply.started":"2022-06-27T11:16:23.981168Z","shell.execute_reply":"2022-06-27T11:16:23.991398Z"},"trusted":true},"execution_count":146,"outputs":[]}]}